{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00|-1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "values:\n",
      "---------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.73| 0.00| 0.00| 0.00|\n",
      "---------------------------\n",
      " 0.66| 0.00| 0.00| 0.00|\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  R  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  R  |  U  |\n"
     ]
    }
   ],
   "source": [
    "# T(0)\n",
    "\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from grid_world import standard_grid, negative_grid\n",
    "from iterative_policy_evaluation import print_values, print_policy\n",
    "\n",
    "SMALL_ENOUGH = 1e-3\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.1\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "# NOTE: this is only policy evaluation, not optimization\n",
    "\n",
    "def random_action(a, eps=0.00000000000000000001):\n",
    "  # we'll use epsilon-soft to ensure all states are visited\n",
    "  # what happens if you don't do this? i.e. eps=0\n",
    "  p = np.random.random()b\n",
    "  if p < (1 - eps):\n",
    "    return a\n",
    "  else:\n",
    "    return np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "def play_game(grid, policy):\n",
    "  # returns a list of states and corresponding rewards (not returns as in MC)\n",
    "  # start at the designated start state\n",
    "  s = (2, 0)\n",
    "  grid.set_state(s)\n",
    "  states_and_rewards = [(s, 0)] # list of tuples of (state, reward)\n",
    "  while not grid.game_over():\n",
    "    a = policy[s]\n",
    "    a = random_action(a)\n",
    "    r = grid.move(a)\n",
    "    s = grid.current_state()\n",
    "    states_and_rewards.append((s, r))\n",
    "  return states_and_rewards\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # use the standard grid again (0 for every step) so that we can compare\n",
    "  # to iterative policy evaluation\n",
    "  grid = standard_grid()\n",
    "\n",
    "  # print rewards\n",
    "  print(\"rewards:\")\n",
    "  print_values(grid.rewards, grid)\n",
    "\n",
    "  # state -> action\n",
    "  policy = {\n",
    "    (2, 0): 'U',\n",
    "    (1, 0): 'U',\n",
    "    (0, 0): 'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2): 'R',\n",
    "    (1, 2): 'R',\n",
    "    (2, 1): 'R',\n",
    "    (2, 2): 'R',\n",
    "    (2, 3): 'U',\n",
    "  }\n",
    "\n",
    "  # initialize V(s) and returns\n",
    "  V = {}\n",
    "  states = grid.all_states()\n",
    "  for s in states:\n",
    "    V[s] = 0\n",
    "\n",
    "  # repeat until convergence\n",
    "  for it in range(1000):\n",
    "\n",
    "    # generate an episode using pi\n",
    "    states_and_rewards = play_game(grid, policy)\n",
    "    # the first (s, r) tuple is the state we start in and 0\n",
    "    # (since we don't get a reward) for simply starting the game\n",
    "    # the last (s, r) tuple is the terminal state and the final reward\n",
    "    # the value for the terminal state is by definition 0, so we don't\n",
    "    # care about updating it.\n",
    "    for t in range(len(states_and_rewards) - 1):\n",
    "      s, _ = states_and_rewards[t]\n",
    "      s2, r = states_and_rewards[t+1]\n",
    "      # we will update V(s) AS we experience the episode\n",
    "      V[s] = V[s] + ALPHA*(r + GAMMA*V[s2] - V[s])\n",
    "\n",
    "  print(\"values:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"policy:\")\n",
    "  print_policy(policy, grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module://ipykernel.pylab.backend_inline\n"
     ]
    }
   ],
   "source": [
    "import matplotlib; import matplotlib.pyplot; print(matplotlib.backends.backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10| 1.00|\n",
      "---------------------------\n",
      "-0.10| 0.00|-0.10|-1.00|\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10|-0.10|\n",
      "it: 0\n",
      "it: 2000\n",
      "it: 4000\n",
      "it: 6000\n",
      "it: 8000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2695377ffd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update counts:\n",
      "---------------------------\n",
      " 0.26| 0.05| 0.04| 0.00|\n",
      "---------------------------\n",
      " 0.13| 0.00| 0.01| 0.00|\n",
      "---------------------------\n",
      " 0.28| 0.07| 0.05| 0.11|\n",
      "values:\n",
      "---------------------------\n",
      " 0.62| 0.80| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.46| 0.00| 0.80| 0.00|\n",
      "---------------------------\n",
      " 0.31| 0.46| 0.62| 0.46|\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "# https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
    "# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from grid_world import standard_grid, negative_grid\n",
    "from iterative_policy_evaluation import print_values, print_policy\n",
    "from monte_carlo_es import max_dict\n",
    "from td0_prediction import random_action\n",
    "\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.1\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # NOTE: if we use the standard grid, there's a good chance we will end up with\n",
    "  # suboptimal policies\n",
    "  # e.g.\n",
    "  # ---------------------------\n",
    "  #   R  |   R  |   R  |      |\n",
    "  # ---------------------------\n",
    "  #   R* |      |   U  |      |\n",
    "  # ---------------------------\n",
    "  #   U  |   R  |   U  |   L  |\n",
    "  # since going R at (1,0) (shown with a *) incurs no cost, it's OK to keep doing that.\n",
    "  # we'll either end up staying in the same spot, or back to the start (2,0), at which\n",
    "  # point we whould then just go back up, or at (0,0), at which point we can continue\n",
    "  # on right.\n",
    "  # instead, let's penalize each movement so the agent will find a shorter route.\n",
    "  #\n",
    "  # grid = standard_grid()\n",
    "  grid = negative_grid(step_cost=-0.1)\n",
    "\n",
    "  # print rewards\n",
    "  print(\"rewards:\")\n",
    "  print_values(grid.rewards, grid)\n",
    "\n",
    "  # no policy initialization, we will derive our policy from most recent Q\n",
    "\n",
    "  # initialize Q(s,a)\n",
    "  Q = {}\n",
    "  states = grid.all_states()\n",
    "  for s in states:\n",
    "    Q[s] = {}\n",
    "    for a in ALL_POSSIBLE_ACTIONS:\n",
    "      Q[s][a] = 0\n",
    "\n",
    "  # let's also keep track of how many times Q[s] has been updated\n",
    "  update_counts = {}\n",
    "  update_counts_sa = {}\n",
    "  for s in states:\n",
    "    update_counts_sa[s] = {}\n",
    "    for a in ALL_POSSIBLE_ACTIONS:\n",
    "      update_counts_sa[s][a] = 1.0\n",
    "\n",
    "  # repeat until convergence\n",
    "  t = 1.0\n",
    "  deltas = []\n",
    "  for it in range(10000):\n",
    "    if it % 100 == 0:\n",
    "      t += 1e-2\n",
    "    if it % 2000 == 0:\n",
    "      print(\"it:\", it)\n",
    "\n",
    "    # instead of 'generating' an epsiode, we will PLAY\n",
    "    # an episode within this loop\n",
    "    s = (2, 0) # start state\n",
    "    grid.set_state(s)\n",
    "\n",
    "    # the first (s, r) tuple is the state we start in and 0\n",
    "    # (since we don't get a reward) for simply starting the game\n",
    "    # the last (s, r) tuple is the terminal state and the final reward\n",
    "    # the value for the terminal state is by definition 0, so we don't\n",
    "    # care about updating it.\n",
    "    a, _ = max_dict(Q[s])\n",
    "    biggest_change = 0\n",
    "    while not grid.game_over():\n",
    "      a = random_action(a, eps=0.5/t) # epsilon-greedy\n",
    "      # random action also works, but slower since you can bump into walls\n",
    "      # a = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "      r = grid.move(a)\n",
    "      s2 = grid.current_state()\n",
    "\n",
    "      # adaptive learning rate\n",
    "      alpha = ALPHA / update_counts_sa[s][a]\n",
    "      update_counts_sa[s][a] += 0.005\n",
    "\n",
    "      # we will update Q(s,a) AS we experience the episode\n",
    "      old_qsa = Q[s][a]\n",
    "      # the difference between SARSA and Q-Learning is with Q-Learning\n",
    "      # we will use this max[a']{ Q(s',a')} in our update\n",
    "      # even if we do not end up taking this action in the next step\n",
    "      a2, max_q_s2a2 = max_dict(Q[s2])\n",
    "      Q[s][a] = Q[s][a] + alpha*(r + GAMMA*max_q_s2a2 - Q[s][a])\n",
    "      biggest_change = max(biggest_change, np.abs(old_qsa - Q[s][a]))\n",
    "\n",
    "      # we would like to know how often Q(s) has been updated too\n",
    "      update_counts[s] = update_counts.get(s,0) + 1\n",
    "\n",
    "      # next state becomes current state\n",
    "      s = s2\n",
    "     \n",
    "    deltas.append(biggest_change)\n",
    "\n",
    "  plt.plot(deltas)\n",
    "  plt.show()\n",
    "\n",
    "  # determine the policy from Q*\n",
    "  # find V* from Q*\n",
    "  policy = {}\n",
    "  V = {}\n",
    "  for s in grid.actions.keys():\n",
    "    a, max_q = max_dict(Q[s])\n",
    "    policy[s] = a\n",
    "    V[s] = max_q\n",
    "\n",
    "  # what's the proportion of time we spend updating each part of Q?\n",
    "  print(\"update counts:\")\n",
    "  total = np.sum(list(update_counts.values()))\n",
    "  for k, v in update_counts.items():\n",
    "    update_counts[k] = float(v) / total\n",
    "  print_values(update_counts, grid)\n",
    "\n",
    "  print(\"values:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"policy:\")\n",
    "  print_policy(policy, grid)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
