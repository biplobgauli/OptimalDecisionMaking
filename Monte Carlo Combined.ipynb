{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00|-1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "values:\n",
      "---------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.73| 0.00|-1.00| 0.00|\n",
      "---------------------------\n",
      " 0.66|-0.81|-0.90|-1.00|\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  R  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  R  |  U  |\n"
     ]
    }
   ],
   "source": [
    "## POLICY EVALUATION STEP-- random policy\n",
    "\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from grid_world import standard_grid, negative_grid\n",
    "from iterative_policy_evaluation import print_values, print_policy\n",
    "\n",
    "SMALL_ENOUGH = 1e-3\n",
    "GAMMA = 0.9\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "# NOTE: this is only policy evaluation, not optimization\n",
    "\n",
    "def play_game(grid, policy):\n",
    "  # returns a list of states and corresponding returns\n",
    "\n",
    "  # reset game to start at a random position\n",
    "  # we need to do this, because given our current deterministic policy\n",
    "  # we would never end up at certain states, but we still want to measure their value\n",
    "  start_states = list(grid.actions.keys())\n",
    "  start_idx = np.random.choice(len(start_states))\n",
    "  grid.set_state(start_states[start_idx])\n",
    "\n",
    "  s = grid.current_state()\n",
    "  states_and_rewards = [(s, 0)] # list of tuples of (state, reward)\n",
    "  while not grid.game_over():\n",
    "    a = policy[s]\n",
    "    r = grid.move(a)\n",
    "    s = grid.current_state()\n",
    "    states_and_rewards.append((s, r))\n",
    "  # calculate the returns by working backwards from the terminal state\n",
    "  G = 0\n",
    "  states_and_returns = []\n",
    "  first = True\n",
    "  for s, r in reversed(states_and_rewards):\n",
    "    # the value of the terminal state is 0 by definition\n",
    "    # we should ignore the first state we encounter\n",
    "    # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
    "    if first:\n",
    "      first = False\n",
    "    else:\n",
    "      states_and_returns.append((s, G))\n",
    "    G = r + GAMMA*G\n",
    "  states_and_returns.reverse() # we want it to be in order of state visited\n",
    "  return states_and_returns\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # use the standard grid again (0 for every step) so that we can compare\n",
    "  # to iterative policy evaluation\n",
    "  grid = standard_grid()\n",
    "\n",
    "  # print rewards\n",
    "  print(\"rewards:\")\n",
    "  print_values(grid.rewards, grid)\n",
    "\n",
    "  # state -> action\n",
    "  policy = {\n",
    "    (2, 0): 'U',\n",
    "    (1, 0): 'U',\n",
    "    (0, 0): 'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2): 'R',\n",
    "    (1, 2): 'R',\n",
    "    (2, 1): 'R',\n",
    "    (2, 2): 'R',\n",
    "    (2, 3): 'U',\n",
    "  }\n",
    "\n",
    "  # initialize V(s) and returns\n",
    "  V = {}\n",
    "  returns = {} # dictionary of state -> list of returns we've received\n",
    "  states = grid.all_states()\n",
    "  for s in states:\n",
    "    if s in grid.actions:\n",
    "      returns[s] = []\n",
    "    else:\n",
    "      # terminal state or state we can't otherwise get to\n",
    "      V[s] = 0\n",
    "\n",
    "  # repeat\n",
    "  for t in range(1000):\n",
    "\n",
    "    # generate an episode using pi\n",
    "    states_and_returns = play_game(grid, policy)\n",
    "    seen_states = set()\n",
    "    for s, G in states_and_returns:\n",
    "      # check if we have already seen s\n",
    "      # called \"first-visit\" MC policy evaluation\n",
    "      if s not in seen_states:\n",
    "        returns[s].append(G)\n",
    "        V[s] = np.mean(returns[s])\n",
    "        seen_states.add(s)\n",
    "\n",
    "  print(\"values:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"policy:\")\n",
    "  print_policy(policy, grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00|-1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "values:\n",
      "---------------------------\n",
      " 0.44| 0.57| 0.73| 0.00|\n",
      "---------------------------\n",
      " 0.33| 0.00| 0.24| 0.00|\n",
      "---------------------------\n",
      " 0.26| 0.20| 0.15|-0.13|\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  L  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "## POLICY EVALUATION STEP-- win policy\n",
    "\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from grid_world import standard_grid, negative_grid\n",
    "from iterative_policy_evaluation import print_values, print_policy\n",
    "\n",
    "SMALL_ENOUGH = 1e-3\n",
    "GAMMA = 0.9\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "# NOTE: this is only policy evaluation, not optimization\n",
    "\n",
    "def random_action(a):\n",
    "  # choose given a with probability 0.5\n",
    "  # choose some other a' != a with probability 0.5/3\n",
    "  p = np.random.random()\n",
    "  if p < 0.5:\n",
    "    return a\n",
    "  else:\n",
    "    tmp = list(ALL_POSSIBLE_ACTIONS)\n",
    "    tmp.remove(a)\n",
    "    return np.random.choice(tmp)\n",
    "\n",
    "def play_game(grid, policy):\n",
    "  # returns a list of states and corresponding returns\n",
    "\n",
    "  # reset game to start at a random position\n",
    "  # we need to do this, because given our current deterministic policy\n",
    "  # we would never end up at certain states, but we still want to measure their value\n",
    "  start_states = list(grid.actions.keys())\n",
    "  start_idx = np.random.choice(len(start_states))\n",
    "  grid.set_state(start_states[start_idx])\n",
    "\n",
    "  s = grid.current_state()\n",
    "  states_and_rewards = [(s, 0)] # list of tuples of (state, reward)\n",
    "  while not grid.game_over():\n",
    "    a = policy[s]\n",
    "    a = random_action(a)\n",
    "    r = grid.move(a)\n",
    "    s = grid.current_state()\n",
    "    states_and_rewards.append((s, r))\n",
    "  # calculate the returns by working backwards from the terminal state\n",
    "  G = 0\n",
    "  states_and_returns = []\n",
    "  first = True\n",
    "  for s, r in reversed(states_and_rewards):\n",
    "    # the value of the terminal state is 0 by definition\n",
    "    # we should ignore the first state we encounter\n",
    "    # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
    "    if first:\n",
    "      first = False\n",
    "    else:\n",
    "      states_and_returns.append((s, G))\n",
    "    G = r + GAMMA*G\n",
    "  states_and_returns.reverse() # we want it to be in order of state visited\n",
    "  return states_and_returns\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # use the standard grid again (0 for every step) so that we can compare\n",
    "  # to iterative policy evaluation\n",
    "  grid = standard_grid()\n",
    "\n",
    "  # print rewards\n",
    "  print(\"rewards:\")\n",
    "  print_values(grid.rewards, grid)\n",
    "\n",
    "  # state -> action\n",
    "  # found by policy_iteration_random on standard_grid\n",
    "  # MC method won't get exactly this, but should be close\n",
    "  # values:\n",
    "  # ---------------------------\n",
    "  #  0.43|  0.56|  0.72|  0.00|\n",
    "  # ---------------------------\n",
    "  #  0.33|  0.00|  0.21|  0.00|\n",
    "  # ---------------------------\n",
    "  #  0.25|  0.18|  0.11| -0.17|\n",
    "  # policy:\n",
    "  # ---------------------------\n",
    "  #   R  |   R  |   R  |      |\n",
    "  # ---------------------------\n",
    "  #   U  |      |   U  |      |\n",
    "  # ---------------------------\n",
    "  #   U  |   L  |   U  |   L  |\n",
    "  policy = {\n",
    "    (2, 0): 'U',\n",
    "    (1, 0): 'U',\n",
    "    (0, 0): 'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2): 'R',\n",
    "    (1, 2): 'U',\n",
    "    (2, 1): 'L',\n",
    "    (2, 2): 'U',\n",
    "    (2, 3): 'L',\n",
    "  }\n",
    "\n",
    "  # initialize V(s) and returns\n",
    "  V = {}\n",
    "  returns = {} # dictionary of state -> list of returns we've received\n",
    "  states = grid.all_states()\n",
    "  for s in states:\n",
    "    if s in grid.actions:\n",
    "      returns[s] = []\n",
    "    else:\n",
    "      # terminal state or state we can't otherwise get to\n",
    "      V[s] = 0\n",
    "\n",
    "  # repeat until convergence\n",
    "  for t in range(5000):\n",
    "\n",
    "    # generate an episode using pi\n",
    "    states_and_returns = play_game(grid, policy)\n",
    "    seen_states = set()\n",
    "    for s, G in states_and_returns:\n",
    "      # check if we have already seen s\n",
    "      # called \"first-visit\" MC policy evaluation\n",
    "      if s not in seen_states:\n",
    "        returns[s].append(G)\n",
    "        V[s] = np.mean(returns[s])\n",
    "        seen_states.add(s)\n",
    "\n",
    "  print(\"values:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"policy:\")\n",
    "  print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10| 1.00|\n",
      "---------------------------\n",
      "-0.10| 0.00|-0.10|-1.00|\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10|-0.10|\n",
      "0\n",
      "1000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f2a1337780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  D  |     |  U  |     |\n",
      "---------------------------\n",
      "  R  |  R  |  U  |  L  |\n",
      "final values:\n",
      "---------------------------\n",
      " 0.62| 0.80| 1.00| 0.00|\n",
      "---------------------------\n",
      "-0.54| 0.00| 0.80| 0.00|\n",
      "---------------------------\n",
      " 0.12| 0.46| 0.62| 0.46|\n"
     ]
    }
   ],
   "source": [
    "## POLICY OPTIMIZATION STEP--god mode\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from grid_world import standard_grid, negative_grid\n",
    "from iterative_policy_evaluation import print_values, print_policy\n",
    "\n",
    "GAMMA = 0.9\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "# NOTE: this script implements the Monte Carlo Exploring-Starts method\n",
    "#       for finding the optimal policy\n",
    "\n",
    "def play_game(grid, policy):\n",
    "  # returns a list of states and corresponding returns\n",
    "\n",
    "  # reset game to start at a random position\n",
    "  # we need to do this if we have a deterministic policy\n",
    "  # we would never end up at certain states, but we still want to measure their value\n",
    "  # this is called the \"exploring starts\" method\n",
    "  start_states = list(grid.actions.keys())\n",
    "  start_idx = np.random.choice(len(start_states))\n",
    "  grid.set_state(start_states[start_idx])\n",
    "\n",
    "  s = grid.current_state()\n",
    "  a = np.random.choice(ALL_POSSIBLE_ACTIONS) # first action is uniformly random\n",
    "\n",
    "  # be aware of the timing\n",
    "  # each triple is s(t), a(t), r(t)\n",
    "  # but r(t) results from taking action a(t-1) from s(t-1) and landing in s(t)\n",
    "  states_actions_rewards = [(s, a, 0)]\n",
    "  seen_states = set()\n",
    "  while True:\n",
    "    old_s = grid.current_state()\n",
    "    r = grid.move(a)\n",
    "    s = grid.current_state()\n",
    "\n",
    "    if s in seen_states:\n",
    "      # hack so that we don't end up in an infinitely long episode\n",
    "      # bumping into the wall repeatedly\n",
    "      states_actions_rewards.append((s, None, -100))\n",
    "      break\n",
    "    elif grid.game_over():\n",
    "      states_actions_rewards.append((s, None, r))\n",
    "      break\n",
    "    else:\n",
    "      a = policy[s]\n",
    "      states_actions_rewards.append((s, a, r))\n",
    "    seen_states.add(s)\n",
    "\n",
    "  # calculate the returns by working backwards from the terminal state\n",
    "  G = 0\n",
    "  states_actions_returns = []\n",
    "  first = True\n",
    "  for s, a, r in reversed(states_actions_rewards):\n",
    "    # the value of the terminal state is 0 by definition\n",
    "    # we should ignore the first state we encounter\n",
    "    # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
    "    if first:\n",
    "      first = False\n",
    "    else:\n",
    "      states_actions_returns.append((s, a, G))\n",
    "    G = r + GAMMA*G\n",
    "  states_actions_returns.reverse() # we want it to be in order of state visited\n",
    "  return states_actions_returns\n",
    "\n",
    "\n",
    "def max_dict(d):\n",
    "  # returns the argmax (key) and max (value) from a dictionary\n",
    "  # put this into a function since we are using it so often\n",
    "  max_key = None\n",
    "  max_val = float('-inf')\n",
    "  for k, v in d.items():\n",
    "    if v > max_val:\n",
    "      max_val = v\n",
    "      max_key = k\n",
    "  return max_key, max_val\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # use the standard grid again (0 for every step) so that we can compare\n",
    "  # to iterative policy evaluation\n",
    "  # grid = standard_grid()\n",
    "  # try the negative grid too, to see if agent will learn to go past the \"bad spot\"\n",
    "  # in order to minimize number of steps\n",
    "  grid = negative_grid(step_cost=-0.1)\n",
    "\n",
    "  # print rewards\n",
    "  print(\"rewards:\")\n",
    "  print_values(grid.rewards, grid)\n",
    "\n",
    "  # state -> action\n",
    "  # initialize a random policy\n",
    "  policy = {}\n",
    "  for s in grid.actions.keys():\n",
    "    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "  # initialize Q(s,a) and returns\n",
    "  Q = {}\n",
    "  returns = {} # dictionary of state -> list of returns we've received\n",
    "  states = grid.all_states()\n",
    "  for s in states:\n",
    "    if s in grid.actions: # not a terminal state\n",
    "      Q[s] = {}\n",
    "      for a in ALL_POSSIBLE_ACTIONS:\n",
    "        Q[s][a] = 0 # needs to be initialized to something so we can argmax it\n",
    "        returns[(s,a)] = []\n",
    "    else:\n",
    "      # terminal state or state we can't otherwise get to\n",
    "      pass\n",
    "\n",
    "  # repeat until convergence\n",
    "  deltas = []\n",
    "  for t in range(2000):\n",
    "    if t % 1000 == 0:\n",
    "      print(t)\n",
    "\n",
    "    # generate an episode using pi\n",
    "    biggest_change = 0\n",
    "    states_actions_returns = play_game(grid, policy)\n",
    "    seen_state_action_pairs = set()\n",
    "    for s, a, G in states_actions_returns:\n",
    "      # check if we have already seen s\n",
    "      # called \"first-visit\" MC policy evaluation\n",
    "      sa = (s, a)\n",
    "      if sa not in seen_state_action_pairs:\n",
    "        old_q = Q[s][a]\n",
    "        returns[sa].append(G)\n",
    "        Q[s][a] = np.mean(returns[sa])\n",
    "        biggest_change = max(biggest_change, np.abs(old_q - Q[s][a]))\n",
    "        seen_state_action_pairs.add(sa)\n",
    "    deltas.append(biggest_change)\n",
    "\n",
    "    # update policy\n",
    "    for s in policy.keys():\n",
    "      policy[s] = max_dict(Q[s])[0]\n",
    "\n",
    "  plt.plot(deltas)\n",
    "  plt.show()\n",
    "\n",
    "  print(\"final policy:\")\n",
    "  print_policy(policy, grid)\n",
    "\n",
    "  # find V\n",
    "  V = {}\n",
    "  for s, Qs in Q.items():\n",
    "    V[s] = max_dict(Q[s])[1]\n",
    "\n",
    "  print(\"final values:\")\n",
    "  print_values(V, grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## POLICY OPTIMIZATION STEP--fixed starting point (2,0)\n",
    "\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from grid_world import standard_grid, negative_grid\n",
    "from iterative_policy_evaluation import print_values, print_policy\n",
    "from monte_carlo_es import max_dict\n",
    "\n",
    "import numpy as np\n",
    "from grid_world import standard_grid, negative_grid\n",
    "from iterative_policy_evaluation import print_values, print_policy\n",
    "\n",
    "GAMMA = 0.9\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "# NOTE: find optimal policy and value function\n",
    "#       using on-policy first-visit MC\n",
    "\n",
    "def random_action(a, eps=0.1):\n",
    "  # choose given a with probability 1 - eps + eps/4\n",
    "  # choose some other a' != a with probability eps/4\n",
    "  p = np.random.random()\n",
    "  # if p < (1 - eps + eps/len(ALL_POSSIBLE_ACTIONS)):\n",
    "  #   return a\n",
    "  # else:\n",
    "  #   tmp = list(ALL_POSSIBLE_ACTIONS)\n",
    "  #   tmp.remove(a)\n",
    "  #   return np.random.choice(tmp)\n",
    "  #\n",
    "  # this is equivalent to the above\n",
    "  if p < (1 - eps):\n",
    "    return a\n",
    "  else:\n",
    "    return np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "def play_game(grid, policy):\n",
    "  # returns a list of states and corresponding returns\n",
    "  # in this version we will NOT use \"exploring starts\" method\n",
    "  # instead we will explore using an epsilon-soft policy\n",
    "  s = (2, 0)\n",
    "  grid.set_state(s)\n",
    "  a = random_action(policy[s])\n",
    "\n",
    "  # be aware of the timing\n",
    "  # each triple is s(t), a(t), r(t)\n",
    "  # but r(t) results from taking action a(t-1) from s(t-1) and landing in s(t)\n",
    "  states_actions_rewards = [(s, a, 0)]\n",
    "  while True:\n",
    "    r = grid.move(a)\n",
    "    s = grid.current_state()\n",
    "    if grid.game_over():\n",
    "      states_actions_rewards.append((s, None, r))\n",
    "      break\n",
    "    else:\n",
    "      a = random_action(policy[s]) # the next state is stochastic\n",
    "      states_actions_rewards.append((s, a, r))\n",
    "\n",
    "  # calculate the returns by working backwards from the terminal state\n",
    "  G = 0\n",
    "  states_actions_returns = []\n",
    "  first = True\n",
    "  for s, a, r in reversed(states_actions_rewards):\n",
    "    # the value of the terminal state is 0 by definition\n",
    "    # we should ignore the first state we encounter\n",
    "    # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
    "    if first:\n",
    "      first = False\n",
    "    else:\n",
    "      states_actions_returns.append((s, a, G))\n",
    "    G = r + GAMMA*G\n",
    "  states_actions_returns.reverse() # we want it to be in order of state visited\n",
    "  return states_actions_returns\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # use the standard grid again (0 for every step) so that we can compare\n",
    "  # to iterative policy evaluation\n",
    "  # grid = standard_grid()\n",
    "  # try the negative grid too, to see if agent will learn to go past the \"bad spot\"\n",
    "  # in order to minimize number of steps\n",
    "  grid = negative_grid(step_cost=-0.1)\n",
    "\n",
    "  # print rewards\n",
    "  print(\"rewards:\")\n",
    "  print_values(grid.rewards, grid)\n",
    "\n",
    "  # state -> action\n",
    "  # initialize a random policy\n",
    "  policy = {}\n",
    "  for s in grid.actions.keys():\n",
    "    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "  # initialize Q(s,a) and returns\n",
    "  Q = {}\n",
    "  returns = {} # dictionary of state -> list of returns we've received\n",
    "  states = grid.all_states()\n",
    "  for s in states:\n",
    "    if s in grid.actions: # not a terminal state\n",
    "      Q[s] = {}\n",
    "      for a in ALL_POSSIBLE_ACTIONS:\n",
    "        Q[s][a] = 0\n",
    "        returns[(s,a)] = []\n",
    "    else:\n",
    "      # terminal state or state we can't otherwise get to\n",
    "      pass\n",
    "\n",
    "  # repeat until convergence\n",
    "  deltas = []\n",
    "  for t in range(5000):\n",
    "    if t % 1000 == 0:\n",
    "      print(t)\n",
    "\n",
    "    # generate an episode using pi\n",
    "    biggest_change = 0\n",
    "    states_actions_returns = play_game(grid, policy)\n",
    "\n",
    "    # calculate Q(s,a)\n",
    "    seen_state_action_pairs = set()\n",
    "    for s, a, G in states_actions_returns:\n",
    "      # check if we have already seen s\n",
    "      # called \"first-visit\" MC policy evaluation\n",
    "      sa = (s, a)\n",
    "      if sa not in seen_state_action_pairs:\n",
    "        old_q = Q[s][a]\n",
    "        returns[sa].append(G)\n",
    "        Q[s][a] = np.mean(returns[sa])\n",
    "        biggest_change = max(biggest_change, np.abs(old_q - Q[s][a]))\n",
    "        seen_state_action_pairs.add(sa)\n",
    "    deltas.append(biggest_change)\n",
    "\n",
    "    # calculate new policy pi(s) = argmax[a]{ Q(s,a) }\n",
    "    for s in policy.keys():\n",
    "      a, _ = max_dict(Q[s])\n",
    "      policy[s] = a\n",
    "\n",
    "  plt.plot(deltas)\n",
    "  plt.show()\n",
    "\n",
    "  # find the optimal state-value function\n",
    "  # V(s) = max[a]{ Q(s,a) }\n",
    "  V = {}\n",
    "  for s in policy.keys():\n",
    "    V[s] = max_dict(Q[s])[1]\n",
    "\n",
    "  print(\"final values:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"final policy:\")\n",
    "  print_policy(policy, grid)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
